{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introdução"
      ],
      "metadata": {
        "id": "NZpvFGowL8a6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O projeto Guard.IAn é um sistema de inteligência artificial voltado à detecção de violência em vídeos e imagens. Utilizando redes neurais convolucionais (CNNs) e técnicas de deep learning, o sistema analisará cenas capturadas por câmeras de segurança para identificar comportamentos agressivos, como brigas e ataques físicos, distinguindo-os de interações normais com maior precisão e menos alarmes falsos.\n",
        "\n",
        "Para isso, serão empregados modelos pré-treinados, como a VGG-16, que atuarão como extratores de características visuais dos frames. A partir de bases de dados rotuladas com cenas violentas e não violentas, o Guard.IAn será treinado e ajustado para reconhecer padrões agressivos, com potencial de integração futura a plataformas de monitoramento e emissão de alertas automáticos em tempo real.\n"
      ],
      "metadata": {
        "id": "XRowSkAkL_uZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Código"
      ],
      "metadata": {
        "id": "e2mXohiyXHZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparando Modelo"
      ],
      "metadata": {
        "id": "L1TYLL5IvoVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importação de Bibliotecas"
      ],
      "metadata": {
        "id": "1wsN-QZhXKcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import collections\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Bibliotecas de terceiros (instaladas via pip)\n",
        "import cv2\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import (Activation, Dense, Dropout, GlobalAveragePooling2D,\n",
        "                                     Input, LSTM)\n",
        "from tensorflow.keras.models import Model, Sequential"
      ],
      "metadata": {
        "id": "2I5sOrbGXMvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extração de Frames"
      ],
      "metadata": {
        "id": "p2t0Gi8pxBr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames(video_path, max_frames=40):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret or count >= max_frames:\n",
        "            break\n",
        "        frame = cv2.resize(frame, (224, 224))\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "        count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Converte para array numpy\n",
        "    frames = np.array(frames)\n",
        "\n",
        "    # Se o vídeo tiver menos de max_frames, preenche com zeros\n",
        "    if len(frames) < max_frames:\n",
        "        pad_length = max_frames - len(frames)\n",
        "        padding = np.zeros((pad_length, 224, 224, 3), dtype=np.uint8)\n",
        "        frames = np.concatenate((frames, padding), axis=0)\n",
        "\n",
        "    return frames"
      ],
      "metadata": {
        "id": "xpVe-rhyxBXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorizando vídeos"
      ],
      "metadata": {
        "id": "i6eFCWF5yBoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_videos_from_folder(folder_path, label, max_frames=40):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.mp4') or filename.endswith('.avi'):\n",
        "            video_path = os.path.join(folder_path, filename)\n",
        "            frames = extract_frames(video_path, max_frames)\n",
        "            if len(frames) == max_frames:\n",
        "                data.append(frames)\n",
        "                labels.append(label)\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "# Caminhos\n",
        "violence_path = os.path.join('data', 'Violence')\n",
        "non_violence_path = os.path.join('data', 'NonViolence')\n",
        "# Carrega vídeos\n",
        "violence_data, violence_labels = load_videos_from_folder(violence_path, label=1)\n",
        "nonviolence_data, nonviolence_labels = load_videos_from_folder(nonviolence_path, label=0)\n",
        "\n",
        "# Junta tudo\n",
        "X = np.array(violence_data + nonviolence_data)\n",
        "y = np.array(violence_labels + nonviolence_labels)\n",
        "\n",
        "# Embaralha os dados\n",
        "from sklearn.utils import shuffle\n",
        "X, y = shuffle(X, y, random_state=42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EiYOsdEyJ6n",
        "outputId": "4f2a06fd-daa4-4dcd-fd0f-e9a3be54ee4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extração de Características com VGG-16"
      ],
      "metadata": {
        "id": "Na0-lIgm8rOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega o modelo VGG16 pré-treinado no ImageNet, excluindo as camadas do topo\n",
        "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Input para o extrator de características\n",
        "feature_extractor_input = Input(shape=(224, 224, 3))\n",
        "\n",
        "# Conecta o VGG16 ao input\n",
        "x = vgg16_model(feature_extractor_input)\n",
        "\n",
        "# Adiciona uma camada de Global Average Pooling 2D\n",
        "feature_extractor_output = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Cria o modelo extrator de características\n",
        "feature_extractor_model = Model(inputs=feature_extractor_input, outputs=feature_extractor_output)\n",
        "\n",
        "print(\"Resumo do Modelo Extrator de Características (VGG16 com GlobalAveragePooling):\")\n",
        "feature_extractor_model.summary()\n",
        "\n",
        "print(\"\\nIniciando a extração de características de vídeo...\")\n",
        "extracted_features = []\n",
        "for i, video_frames in enumerate(X):\n",
        "    video_features = []\n",
        "    for frame in video_frames:\n",
        "        frame_features = feature_extractor_model.predict(np.expand_dims(frame, axis=0), verbose=0)[0]\n",
        "        video_features.append(frame_features)\n",
        "    extracted_features.append(video_features)\n",
        "    if (i + 1) % 10 == 0:\n",
        "        print(f\"Extração de características concluída para {i + 1}/{len(X)} vídeos.\")\n",
        "\n",
        "extracted_features = np.array(extracted_features)\n",
        "\n",
        "print(f\"\\nExtração de características concluída. Formato final: {extracted_features.shape}\")"
      ],
      "metadata": {
        "id": "AdaQHiVO8wvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinando Modelo"
      ],
      "metadata": {
        "id": "VgIP623AvflL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construção de modelo LTSM"
      ],
      "metadata": {
        "id": "RXjeQMuz_fS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir as dimensões de entrada para o modelo LSTM\n",
        "MAX_FRAMES = extracted_features.shape[1]\n",
        "FEATURE_DIM = extracted_features.shape[2]\n",
        "\n",
        "print(f\"Dimensões de entrada para o LSTM: (max_frames={MAX_FRAMES}, feature_dimension={FEATURE_DIM})\")\n",
        "\n",
        "# Construção do Modelo Sequencial\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(units=256, return_sequences=True, input_shape=(MAX_FRAMES, FEATURE_DIM)))\n",
        "model.add(Dropout(0.6))\n",
        "\n",
        "model.add(LSTM(units=64, return_sequences=False))\n",
        "model.add(Dropout(0.6))\n",
        "\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dropout(0.6))\n",
        "\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compilação do Modelo\n",
        "adam = optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Exibe o resumo do modelo\n",
        "print(\"\\nResumo do Modelo LSTM:\")\n",
        "model.summary()\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,                # Para o treino se não houver melhora por 5 épocas\n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "JWpDia-e_j73",
        "outputId": "67507992-8762-4aac-a3d0-7d24f13bfd61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensões de entrada para o LSTM: (max_frames=40, feature_dimension=512)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resumo do Modelo LSTM:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m787,456\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m82,176\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">787,456</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">82,176</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m871,745\u001b[0m (3.33 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">871,745</span> (3.33 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m871,745\u001b[0m (3.33 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">871,745</span> (3.33 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinamento do Modelo"
      ],
      "metadata": {
        "id": "_IQwuVTtC9Cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    extracted_features, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Shape de X_train: {X_train.shape}\")\n",
        "print(f\"Shape de y_train: {y_train.shape}\")\n",
        "print(f\"Shape de X_test: {X_test.shape}\")\n",
        "print(f\"Shape de y_test: {y_test.shape}\")\n",
        "\n",
        "print(\"\\nIniciando o treinamento do modelo...\")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "print(\"\\nTreinamento concluído!\")"
      ],
      "metadata": {
        "id": "dBhTpd5WC-tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Salvando e Carregando Modelo"
      ],
      "metadata": {
        "id": "RJZ26meNvsE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Salvar Modelo"
      ],
      "metadata": {
        "id": "hztOqk3NHQIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Garante que a pasta 'models' exista no ambiente local\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "model.save('models/guardian_model.h5')\n",
        "print(\"Modelo salvo localmente na pasta 'models'.\")"
      ],
      "metadata": {
        "id": "2mspLmaVHT67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carregar Modelo"
      ],
      "metadata": {
        "id": "ZQwbxQUMvDHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('models/guardian_model.h5')\n",
        "print(\"Modelo pré-treinado carregado com sucesso!\")"
      ],
      "metadata": {
        "id": "Ak1gdBTvu9Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação do Modelo"
      ],
      "metadata": {
        "id": "ik8FrD9oDGMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide as CARACTERÍSTICAS para o teste\n",
        "_, X_test, _, y_test = train_test_split(\n",
        "    extracted_features, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Divide os PIXELS originais para podermos exibir os frames depois\n",
        "_, X_test_pixels, _, _ = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "\n",
        "# AVALIAÇÃO DO MODELO E RELATÓRIOS\n",
        "# Avalia o modelo no conjunto de teste\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Acurácia do modelo no conjunto de teste: {accuracy:.4f}\")\n",
        "print(f\"Perda do modelo no conjunto de teste: {loss:.4f}\")\n",
        "\n",
        "y_pred_proba = model.predict(X_test, verbose=0)\n",
        "y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "print(\"\\nRelatório de Classificação:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nMatriz de Confusão:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "\n",
        "# PLOT DOS GRÁFICOS DE TREINAMENTO\n",
        "print(\"\\nVisualizando o Histórico de Treinamento...\")\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Plot da Acurácia\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Acurácia de Treinamento')\n",
        "plt.plot(history.history['val_accuracy'], label='Acurácia de Validação')\n",
        "plt.title('Acurácia do Modelo ao Longo das Épocas')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Acurácia')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot da Perda\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Perda de Treinamento')\n",
        "plt.plot(history.history['val_loss'], label='Perda de Validação')\n",
        "plt.title('Perda do Modelo ao Longo das Épocas')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Perda')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# EXIBIÇÃO DOS FRAMES DE EXEMPLO\n",
        "\n",
        "# Encontrar um exemplo de VIOLÊNCIA detectado corretamente (Verdadeiro Positivo)\n",
        "tp_indices = np.where((y_test == 1) & (y_pred.flatten() == 1))[0]\n",
        "# Encontrar um exemplo de NÃO VIOLÊNCIA detectado corretamente (Verdadeiro Negativo)\n",
        "tn_indices = np.where((y_test == 0) & (y_pred.flatten() == 0))[0]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Exibe o frame de VIOLÊNCIA\n",
        "if len(tp_indices) > 0:\n",
        "    idx = np.random.choice(tp_indices)\n",
        "    frame_violento = X_test_pixels[idx][20] # Pega um frame do meio do vídeo\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(frame_violento)\n",
        "    plt.title('Exemplo de Detecção Correta de VIOLÊNCIA')\n",
        "    plt.axis('off')\n",
        "\n",
        "# Exibe o frame de NÃO VIOLÊNCIA\n",
        "if len(tn_indices) > 0:\n",
        "    idx = np.random.choice(tn_indices)\n",
        "    frame_nao_violento = X_test_pixels[idx][20] # Pega um frame do meio do vídeo\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(frame_nao_violento)\n",
        "    plt.title('Exemplo de Detecção Correta de NÃO VIOLÊNCIA')\n",
        "    plt.axis('off')\n",
        "\n",
        "\n",
        "if len(tp_indices) > 0 or len(tn_indices) > 0:\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "q_R15TyLDHun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criação de Janela Deslizante Para Análise Inteiriça do Vídeo"
      ],
      "metadata": {
        "id": "T-xpH7fdi3jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analisar_video_com_janela_deslizante(video_path, feature_extractor, lstm_classifier,\n",
        "                                           window_size=40, step=10, confidence_threshold=0.80):\n",
        "    \"\"\"\n",
        "    Analisa um vídeo usando uma janela deslizante para detectar violência.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Iniciando Análise Contínua do Vídeo: {os.path.basename(video_path)} ---\")\n",
        "\n",
        "    try:\n",
        "        video_stream = cv2.VideoCapture(video_path)\n",
        "        if not video_stream.isOpened():\n",
        "            print(\"Erro ao abrir o arquivo de vídeo.\")\n",
        "            return\n",
        "\n",
        "        fps = video_stream.get(cv2.CAP_PROP_FPS)\n",
        "        frames_deque = collections.deque(maxlen=window_size)\n",
        "        frame_count = 0\n",
        "        violence_detected = False\n",
        "\n",
        "        while True:\n",
        "            ret, frame = video_stream.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            frame_count += 1\n",
        "            frame_resized = cv2.resize(frame, (224, 224))\n",
        "            frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
        "            frames_deque.append(frame_rgb)\n",
        "\n",
        "            if len(frames_deque) == window_size and (frame_count % step) == 0:\n",
        "                features_para_predicao = []\n",
        "                for f in list(frames_deque):\n",
        "                    frame_features = feature_extractor.predict(np.expand_dims(f, axis=0), verbose=0)[0]\n",
        "                    features_para_predicao.append(frame_features)\n",
        "\n",
        "                processed_video_features = np.array(features_para_predicao)\n",
        "                processed_video_features = np.expand_dims(processed_video_features, axis=0)\n",
        "                prediction_proba = lstm_classifier.predict(processed_video_features, verbose=0)[0][0]\n",
        "\n",
        "                if prediction_proba > confidence_threshold:\n",
        "                    timestamp = frame_count / fps\n",
        "                    print(f\"\\n>>> ALERTA: Violência detectada em {timestamp:.2f} segundos! <<<\")\n",
        "                    print(f\"    (Probabilidade: {prediction_proba:.2f})\")\n",
        "                    violence_detected = True\n",
        "                    break\n",
        "\n",
        "        if not violence_detected:\n",
        "            print(\"\\n--- Análise concluída. Nenhuma violência detectada. ---\")\n",
        "\n",
        "    finally:\n",
        "        if 'video_stream' in locals() and video_stream.isOpened():\n",
        "            video_stream.release()"
      ],
      "metadata": {
        "id": "2cLsdmkqjDI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testes com vídeos aleatórios foras do dataset"
      ],
      "metadata": {
        "id": "9vkARCxHZF4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analisar_video_com_janela_deslizante(video_path, feature_extractor, lstm_classifier,\n",
        "                                           window_size=40, step=10, confidence_threshold=0.80):\n",
        "    \"\"\"\n",
        "    Analisa um vídeo usando uma janela deslizante e exibe o frame em caso de detecção,\n",
        "    ou um frame aleatório se nada for encontrado.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n--- Iniciando Análise Contínua do Vídeo: {os.path.basename(video_path)} ---\")\n",
        "\n",
        "    try:\n",
        "        video_stream = cv2.VideoCapture(video_path)\n",
        "        if not video_stream.isOpened():\n",
        "            print(\"Erro ao abrir o arquivo de vídeo.\")\n",
        "            return\n",
        "\n",
        "        fps = video_stream.get(cv2.CAP_PROP_FPS)\n",
        "        total_frames = int(video_stream.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        frames_deque = collections.deque(maxlen=window_size)\n",
        "        frame_count = 0\n",
        "        violence_detected = False\n",
        "\n",
        "        while True:\n",
        "            ret, frame = video_stream.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "            frame_resized = cv2.resize(frame, (224, 224))\n",
        "            frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
        "            frames_deque.append(frame_rgb)\n",
        "\n",
        "            if len(frames_deque) == window_size and (frame_count % step) == 0:\n",
        "                features_para_predicao = []\n",
        "                for f in list(frames_deque):\n",
        "                    frame_features = feature_extractor.predict(np.expand_dims(f, axis=0), verbose=0)[0]\n",
        "                    features_para_predicao.append(frame_features)\n",
        "\n",
        "                processed_video_features = np.array(features_para_predicao)\n",
        "                processed_video_features = np.expand_dims(processed_video_features, axis=0)\n",
        "                prediction_proba = lstm_classifier.predict(processed_video_features, verbose=0)[0][0]\n",
        "\n",
        "                if prediction_proba > confidence_threshold:\n",
        "                    timestamp = frame_count / fps\n",
        "                    print(f\"\\n>>> ALERTA: Violência detectada em {timestamp:.2f} segundos! <<<\")\n",
        "                    print(f\"    (Probabilidade: {prediction_proba:.2f})\")\n",
        "\n",
        "                    print(\"\\nFrame que acionou o alerta:\")\n",
        "                    frame_display = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                    plt.figure(figsize=(10, 8))\n",
        "                    plt.imshow(frame_display)\n",
        "                    plt.axis('off')\n",
        "                    plt.show()\n",
        "\n",
        "                    violence_detected = True\n",
        "                    break\n",
        "\n",
        "        if not violence_detected:\n",
        "            print(\"\\n--- Análise concluída. Nenhuma violência detectada. ---\")\n",
        "\n",
        "            if total_frames > 0:\n",
        "                random_frame_index = random.randint(0, total_frames - 1)\n",
        "                # É necessário reabrir para pular para o frame, pois o stream já foi lido\n",
        "                video_stream_rand = cv2.VideoCapture(video_path)\n",
        "                if video_stream_rand.isOpened():\n",
        "                    video_stream_rand.set(cv2.CAP_PROP_POS_FRAMES, random_frame_index)\n",
        "                    ret_rand, random_frame = video_stream_rand.read()\n",
        "\n",
        "                    if ret_rand:\n",
        "                        print(f\"\\nExibindo um frame aleatório (frame nº {random_frame_index}):\")\n",
        "                        frame_display = cv2.cvtColor(random_frame, cv2.COLOR_BGR2RGB)\n",
        "                        plt.figure(figsize=(10, 8))\n",
        "                        plt.imshow(frame_display)\n",
        "                        plt.axis('off')\n",
        "                        plt.show()\n",
        "                    video_stream_rand.release()\n",
        "\n",
        "    finally:\n",
        "        if 'video_stream' in locals() and video_stream.isOpened():\n",
        "            video_stream.release()\n",
        "\n",
        "# 2. EXECUÇÃO DO TESTE\n",
        "test_videos_path = 'videos_para_testar'\n",
        "print(f\"Buscando vídeos na pasta: '{test_videos_path}'\")\n",
        "\n",
        "# Verifica se a pasta de teste realmente existe\n",
        "if not os.path.isdir(test_videos_path):\n",
        "    print(f\"\\nERRO: A pasta '{test_videos_path}' não foi encontrada.\")\n",
        "    print(\"Por favor, crie a pasta no mesmo nível do notebook e adicione seus vídeos.\")\n",
        "else:\n",
        "    # Lista todos os vídeos disponíveis na pasta\n",
        "    available_videos = [os.path.join(test_videos_path, f) for f in os.listdir(test_videos_path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
        "\n",
        "    # Verifica se encontrou algum vídeo\n",
        "    if not available_videos:\n",
        "        print(f\"\\nNenhum vídeo encontrado na pasta '{test_videos_path}'.\")\n",
        "        print(\"Por favor, adicione um ou mais vídeos para que o modelo possa ser testado.\")\n",
        "    else:\n",
        "        print(f\"\\nEncontrados {len(available_videos)} vídeo(s). Iniciando análise...\")\n",
        "\n",
        "        # Itera sobre CADA vídeo encontrado na pasta\n",
        "        for video_path in available_videos:\n",
        "            analisar_video_com_janela_deslizante(\n",
        "                video_path=video_path,\n",
        "                feature_extractor=feature_extractor_model,\n",
        "                lstm_classifier=model,\n",
        "                step=15,\n",
        "                confidence_threshold=0.85\n",
        "            )\n",
        "            print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "7Onh2lu1ZJ2W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}